{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the following parameters in this cell\n",
    "# Then run the entire notebook\n",
    "import pm4py\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xmltodict\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "## folder name where a list of xml files are stored\n",
    "## these will be concatenated to one event log file for the given process type\n",
    "\n",
    "## filename of xml input\n",
    "#folderPath = \"./OriginalData/Berlin\"\n",
    "#folderPath = \"./OriginalData/Baden-Württemberg/\"\n",
    "folderPath = \"./OriginalData/Brandenburg\"\n",
    "\n",
    "files = [join(folderPath, f) for f in listdir(folderPath) if isfile(join(folderPath, f))]\n",
    "\n",
    "#print(files)\n",
    "\n",
    "## output filename\n",
    "#outputFilename = './all-data-xes/berlin-all' + '.xes'\n",
    "#outputFilename = \"./all-data-xes/baden-württemberg-all.xes\"\n",
    "outputFilename = \"./all-data-xes/brandenburg-all.xes\"\n",
    "\n",
    "outputFilenamePreprocessed = outputFilename.replace(\".xes\", \"-preprocessed.xes\")\n",
    "\n",
    "FIRST_VALID_YEAR = 1984\n",
    "LAST_VALID_YEAR = 2024\n",
    "\n",
    "MAX_YEARS_DURATION = 5\n",
    "MISSING_ACTIVITY_REPLACEMENT = \"no DokTypL\"\n",
    "MISSING_DATE_REPLACEMENT = pd.Timestamp('1970-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into dict\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='xml_parsing_errors.log', level=logging.ERROR)\n",
    "\n",
    "\n",
    "processes = []\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(file, encoding='utf-8', mode='r') as xml_file:\n",
    "            data = xmltodict.parse(xml_file.read())\n",
    "            dataVorgaenge = data[\"Export\"][\"Vorgang\"] # this has the actual data\n",
    "            processes.extend(dataVorgaenge) # append all processes to the list\n",
    "    except xmltodict.expat.ExpatError as e:\n",
    "        logging.error(f\"Error parsing file {file}: {e}\")\n",
    "        print(f\"Error parsing file {file}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error with file {file}: {e}\")\n",
    "        print(f\"Unexpected error with file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['VNr', 'VFunktion', 'ReihNr', 'VTyp', 'VTypL', 'VSys', 'VSysL', 'VIR',\n",
      "       'Nebeneintrag', 'Dokument'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "VTyp\n",
      "Anfrage                34831\n",
      "Antrag                  4088\n",
      "Bericht                 1685\n",
      "Beschlussempfehlung       73\n",
      "Debatte                19574\n",
      "Gesetz                  1853\n",
      "Sonstiges               6674\n",
      "Vorschrift                33\n",
      "Wahl                     781\n",
      "Name: VTyp, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get all processes into a dataframe\n",
    "processesDF = pd.DataFrame.from_dict(processes)\n",
    "# drop all rows that do not have a value for Dokument\n",
    "# todo: check out what these rows are!\n",
    "processesDF = processesDF.dropna(subset=['Dokument'])\n",
    "\n",
    "print(processesDF.keys())\n",
    "print(\"\\n\")\n",
    "\n",
    "groupedByType = processesDF.groupby(['VTyp'])\n",
    "print(groupedByType['VTyp'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsOfVorgaenge = processesDF[\"Dokument\"]\n",
    "vSysLOfVorgange = processesDF[\"VSysL\"]\n",
    "vSysOfVorgange = processesDF[\"VSys\"]\n",
    "vorgangNebeneintraege = processesDF[\"Nebeneintrag\"]\n",
    "vTypes = processesDF[\"VTyp\"]\n",
    "vTypesL = processesDF[\"VTypL\"]\n",
    "\n",
    "# for a Vorgang there are several Deskriptoren stored in several Nebeneintraege\n",
    "# -> get that information per Vorgang\n",
    "vorgangDeskriptoren = []\n",
    "for nebeneintraege in vorgangNebeneintraege:\n",
    "    if type(nebeneintraege) is not list:\n",
    "        if nebeneintraege is not dict:\n",
    "            deskriptoren = []\n",
    "        else:\n",
    "            deskriptoren = [nebeneintraege.get(\"Desk\", None)]\n",
    "    else:\n",
    "        deskriptoren = [obj.get(\"Desk\", None) for obj in nebeneintraege]\n",
    "\n",
    "    vorgangDeskriptoren.append([x for x in deskriptoren if x is not None])\n",
    "\n",
    "allDocs = []\n",
    "\n",
    "# add trace id to each document\n",
    "# so that the documents can then be single events that belong to a specific trace in an event log\n",
    "\n",
    "for idx, vorgang in enumerate(docsOfVorgaenge):\n",
    "    helperX = vorgang\n",
    "    if type(vorgang) is dict:\n",
    "        helperX = [vorgang]\n",
    "        \n",
    "    dokTypLOfFirstDoc = None\n",
    "    for i, doc in enumerate(helperX):\n",
    "        # add case id column to each document\n",
    "        # call it case:concept:name as this is the name for pm4py transformation into XES format\n",
    "        doc['case:concept:name'] = idx\n",
    "        \n",
    "        # if it is the first document in a Vorgang, then set the dokTypLOfFirstDoc - this could also be used to see what type of process a Vorgang is\n",
    "        if (i == 0):\n",
    "            dokTypLOfFirstDoc = doc.get(\"DokTypL\", \"none\")\n",
    "        \n",
    "        doc['case:DokTypLFirstDoc'] = dokTypLOfFirstDoc\n",
    "\n",
    "        doc['case:VSys'] = vSysOfVorgange.iloc[idx]\n",
    "        doc['case:VSysL'] = vSysLOfVorgange.iloc[idx]\n",
    "        doc['case:VorgangsDeskriptoren'] = vorgangDeskriptoren[idx]\n",
    "        doc[\"case:VTyp\"] = vTypes.iloc[idx]\n",
    "        doc[\"case:VTypL\"] = vTypesL.iloc[idx]\n",
    "    \n",
    "        # write none into DokTypL if it is not defined, so that it will be written to the event log\n",
    "        # if there is no valid string, then it would not be included in the event log\n",
    "        if ('DokTypL' not in doc.keys()):\n",
    "            doc['DokTypL'] = MISSING_ACTIVITY_REPLACEMENT\n",
    "        else:\n",
    "            if doc['DokTypL'] is None:\n",
    "                doc['DokTypL'] = MISSING_ACTIVITY_REPLACEMENT\n",
    "\n",
    "        # if there is no value for a key, replace it with \"none\"\n",
    "        # or if a list value has none values\n",
    "        for key, value in doc.items():\n",
    "            if value is None: \n",
    "                doc.update({key: \"none\"})\n",
    "            if type(value) is list:\n",
    "                doc.update({key: [x if x is not None else \"none\" for x in value]})\n",
    "        \n",
    "        # for grouping reasons also sort all other entries that can be sorted\n",
    "        doc_sortedEntries = {key: sorted(value) if type(value) is list else value for key, value in doc.items()}\n",
    "        \n",
    "        # now add this doc as a row \n",
    "        allDocs.append(doc)\n",
    "\n",
    "\n",
    "\n",
    "# turn docs into data frame, so each row is a document now\n",
    "df = pd.DataFrame(allDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cases overall: 69740\n",
      "number of cases with missing date: 60\n",
      "number of cases with missing or invalid date: 61\n",
      "number of cases with missing activity name: 8128\n",
      "case ids: [    0     1     2 ... 69737 69738 69739]\n"
     ]
    }
   ],
   "source": [
    "print(\"number of cases overall:\", len(df[\"case:concept:name\"].unique()))\n",
    "\n",
    "# turn date string into date time object\n",
    "df[\"DokDat_original\"] = df[\"DokDat\"].copy().astype(str)\n",
    "\n",
    "# check how many traces have a missing date\n",
    "print(\"number of cases with missing date:\", len(df[df[\"DokDat\"].isna()][\"case:concept:name\"].unique()))\n",
    "\n",
    "df[\"DokDat\"] = pd.to_datetime(df['DokDat'], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "# now check how many traces have a missing date including the dates that are invalid due to out of range or wrong format\n",
    "print(\"number of cases with missing or invalid date:\", len(df[df[\"DokDat\"].isna()][\"case:concept:name\"].unique()))\n",
    "\n",
    "# fill missing or invalid dates with a specific default date that is clearly out of scope\n",
    "df[\"DokDat\"].fillna(MISSING_DATE_REPLACEMENT, inplace=True)\n",
    "\n",
    "print(\"number of cases with missing activity name:\", len(df[df[\"DokTypL\"] == MISSING_ACTIVITY_REPLACEMENT][\"case:concept:name\"].unique()))\n",
    "print(\"case ids:\", df[df[\"DokTypL\"] == MISSING_ACTIVITY_REPLACEMENT][\"case:concept:name\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02fd6c5c970470780e63cbc7cfcdb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/69740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use pm4py to create XES file\n",
    "event_log = pm4py.format_dataframe(df, case_id='case:concept:name', activity_key='DokTypL', timestamp_key='DokDat')\n",
    "pm4py.write_xes(event_log, outputFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "Removing traces with missing dates or invalid dates, and cycle time that is too high (also points to invalid dates) - also remove traces with missing activity names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases with an invalid date including out of scope for this data set specifically: 61\n"
     ]
    }
   ],
   "source": [
    "# get all traces with documents with no date and all traces with documents with invalid dates\n",
    "caseIdsInvalidDate = df[(\n",
    "    ((df[\"DokDat\"].dt.year < FIRST_VALID_YEAR) | (df[\"DokDat\"].dt.year > LAST_VALID_YEAR))\n",
    "                        )][\"case:concept:name\"].unique()\n",
    "\n",
    "print(\"cases with an invalid date including out of scope for this data set specifically:\", len(caseIdsInvalidDate))\n",
    "#print(\"trace dates of invalid case:\\n\", df[df[\"case:concept:name\"].isin(caseIdsInvalidDate)][\"DokDat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of cases with a missing activityName before replacement with DokArtL: 8128\n",
      "DokArtL\n",
      "Unterrichtung                           3104\n",
      "Information                             1799\n",
      "Plenarprotokoll                         1250\n",
      "Frühwarndokument                        1209\n",
      "Zuschrift                                895\n",
      "Gutachten                                245\n",
      "Gesetz- und Verordnungsblatt             105\n",
      "Ausschussprotokoll                        82\n",
      "Übersicht                                 23\n",
      "Drucksache                                20\n",
      "Informationen zu Rechtsentwicklungen      17\n",
      "Name: count, dtype: int64\n",
      "no. of cases with a missing activityName after replacement with DokArtL: 78\n",
      "number of cases to remove due to missing/invalid date and missing activity name: 83\n",
      "number of cases after preprocessing: 69657\n"
     ]
    }
   ],
   "source": [
    "# get all traces with missing activity name\n",
    "caseIdsNoActivityName = df[df['DokTypL'] == MISSING_ACTIVITY_REPLACEMENT][\"case:concept:name\"].unique()\n",
    "print(\"no. of cases with a missing activityName before replacement with DokArtL:\", len(caseIdsNoActivityName))\n",
    "\n",
    "# Filter the dataframe to get rows with case IDs in caseIdsNoActivityName\n",
    "df_no_activity_name = df[df['DokTypL'] == MISSING_ACTIVITY_REPLACEMENT]\n",
    "\n",
    "# Get counts of the existing DokArtL values\n",
    "dokArtL_counts = df_no_activity_name[\"DokArtL\"].value_counts()\n",
    "print(dokArtL_counts)\n",
    "# this does not count NaN - the rest of the values are NaN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Replace 'concept:name' with 'DokArtL' if 'concept:name' is MISSING_ACTIVITY_REPLACEMENT and 'DokArtL' is not \"Drucksache\"\n",
    "df.loc[(df['DokTypL'] == MISSING_ACTIVITY_REPLACEMENT) & (df['DokArtL']) & (df['DokArtL'] != \"Drucksache\"), 'DokTypL'] = df['DokArtL']\n",
    "\n",
    "\n",
    "caseIdsNoActivityName = df[df['DokTypL'] == MISSING_ACTIVITY_REPLACEMENT][\"case:concept:name\"].unique()\n",
    "print(\"no. of cases with a missing activityName after replacement with DokArtL:\", len(caseIdsNoActivityName))\n",
    "\n",
    "\n",
    "allCaseIdsToRemove = list(set(list(caseIdsInvalidDate) + list(caseIdsNoActivityName)))\n",
    "print(\"number of cases to remove due to missing/invalid date and missing activity name:\", len(allCaseIdsToRemove))\n",
    "\n",
    "# remove traces with at least one event with no activity name\n",
    "df_preProcessed = df[~df[\"case:concept:name\"].isin(caseIdsInvalidDate)]\n",
    "df_preProcessed = df_preProcessed[~df_preProcessed[\"case:concept:name\"].isin(caseIdsNoActivityName)]\n",
    "\n",
    "\n",
    "print(\"number of cases after preprocessing:\", len(df_preProcessed[\"case:concept:name\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed due to exceeding cycle time limit: 0\n",
      "Unique cases removed due to exceeding cycle time limit: 0\n"
     ]
    }
   ],
   "source": [
    "event_log_preProcessed = pm4py.format_dataframe(df_preProcessed, case_id='case:concept:name', activity_key='DokTypL', timestamp_key='DokDat')\n",
    "\n",
    "# remove traces with unrealistically high duration\n",
    "event_log_removed_high_duration = pm4py.filtering.filter_case_performance(event_log_preProcessed, min_performance=-1, max_performance=365*24*60*60 * MAX_YEARS_DURATION)\n",
    "print(f\"Rows removed due to exceeding cycle time limit: {len(event_log_preProcessed) - len(event_log_removed_high_duration)}\")\n",
    "unique_cases_removed = len(event_log_preProcessed['case:concept:name'].unique()) - len(event_log_removed_high_duration['case:concept:name'].unique())\n",
    "print(f\"Unique cases removed due to exceeding cycle time limit: {unique_cases_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760380f701c8422d988f7fc638cb2135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/69657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use pm4py to create XES file\n",
    "pm4py.write_xes(event_log_removed_high_duration, outputFilenamePreprocessed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
